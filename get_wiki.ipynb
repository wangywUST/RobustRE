{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66de37f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is interrupted.\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "curl -LO https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e833f17b-af09-4b5a-8a16-45ba7ccf1617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import re, regex\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be4bf04b-0b61-427a-8e30-17331f0864b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGEX_PAGE_START = regex.compile('^\\s+\\<page\\>')\n",
    "REGEX_PAGE_END = regex.compile('^\\s+\\</page\\>')\n",
    "REGEX_TITLE = regex.compile('<title>(.+)</title>')\n",
    "REGEX_PARENTHESE = re.compile('(ï¼ˆ|\\().*')\n",
    "\n",
    "def clean_name(name):\n",
    "    name = REGEX_PARENTHESE.sub('', name)\n",
    "    return name.strip()\n",
    "\n",
    "def save(names, path):\n",
    "    with open(path, 'w') as f:\n",
    "        rows = set(f'{name}\\n' for name in names)\n",
    "        f.writelines(rows)\n",
    "\n",
    "def extract(input_path, output_path, REGEX_CATEGORY):\n",
    "    entity_ls = []\n",
    "    page_texts = []\n",
    "    for line in tqdm(iter_wikidata(input_path)):\n",
    "        if REGEX_PAGE_START.search(line):\n",
    "            page_texts = []\n",
    "            continue\n",
    "\n",
    "        page_texts.append(line)\n",
    "        if REGEX_PAGE_END.search(line):\n",
    "            page = ''.join(page_texts)\n",
    "\n",
    "            if not REGEX_CATEGORY.search(page):\n",
    "                continue\n",
    "\n",
    "            title = REGEX_TITLE.search(page).group(1)\n",
    "            name = clean_name(title)\n",
    "\n",
    "            if not is_valid_name(name, only_kanji_hiragana, exclude_short_name):\n",
    "                continue\n",
    "            entity_ls.append(name)\n",
    "\n",
    "    save(entity_ls, output_path)\n",
    "\n",
    "def iter_wikidata(input_path):\n",
    "    if not input_path.endswith('bz2'):\n",
    "        raise ValueError('extention of the input file must be bz2.')\n",
    "\n",
    "    with bz2.BZ2File(input_path) as f:\n",
    "        for line in f:\n",
    "            yield line.decode('utf-8')\n",
    "\n",
    "REGEX_CATEGORY = regex.compile('\\[\\[Category:Organizations')\n",
    "extract('enwiki-latest-pages-articles.xml.bz2', 'output_organization.file')\n",
    "\n",
    "REGEX_CATEGORY = regex.compile('\\[\\[Category:Living_people')\n",
    "extract('enwiki-latest-pages-articles.xml.bz2', 'output_person.file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "220528dd-781f-48dd-94cb-7acfc9e161ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isCharAndSpace(string):\n",
    "    return all(char.isalpha() or char.isspace() for char in string)\n",
    "\n",
    "file_org = open('output_organization.file', 'r')\n",
    "line_org = file_org.readlines()\n",
    "\n",
    "file_per = open('output_person.file', 'r')\n",
    "line_per = file_per.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d173717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l\n"
     ]
    }
   ],
   "source": [
    "print(line_org[0].strip()[-1])\n",
    "name_org = list(set([name_.strip() for name_ in line_org if isCharAndSpace(name_.strip())]))\n",
    "name_per = list(set([name_.strip() for name_ in line_per if isCharAndSpace(name_.strip())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa304262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24933\n",
      "902007\n"
     ]
    }
   ],
   "source": [
    "print(len(name_org))\n",
    "print(len(name_per))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73a0497a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wiki_organization.output']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(name_per, 'wiki_person.output')\n",
    "joblib.dump(name_org, 'wiki_organization.output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3e379a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
